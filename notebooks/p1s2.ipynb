{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1, Step 2: Model & HGC Core Architecture Build\n",
    "\n",
    "**Objective:** To construct the core HGC module and integrate it into a `bert-base-uncased` model, creating our experimental architecture.\n",
    "\n",
    "This notebook will:\n",
    "1. Import the `HolographicKnowledgeManifold` class from our new `hgc_core` module.\n",
    "2. Define a custom PyTorch model, `BertForMaskedLM_With_HGC`, that wraps the standard BERT model.\n",
    "3. Add the HKM and a projection head to the custom model.\n",
    "4. Instantiate both the baseline and HGC-augmented models to verify the architecture is built correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies\n",
    "\n",
    "We import `torch`, `transformers`, and our custom HGC module. We also define key configuration parameters for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertForMaskedLM, BertConfig\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root to the Python path to allow importing from hgc_core\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from hgc_core.hgc_module import HolographicKnowledgeManifold\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "HKM_DIMENSIONALITY = 4096 # The 'd' for our holographic vectors.\n",
    "\n",
    "print(\"Dependencies loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining the HGC-Augmented BERT Architecture\n",
    "\n",
    "Here, we define the custom PyTorch model class. It will hold the pre-trained BERT model, an HKM instance, and a linear layer to project BERT's outputs into the high-dimensional space of the HKM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom HGC-BERT model class defined.\n"
     ]
    }
   ],
   "source": [
    "class BertForMaskedLM_With_HGC(nn.Module):\n",
    "    \"\"\"\n",
    "    A custom BERT model for Masked Language Modeling that integrates a\n",
    "    Holographic Knowledge Manifold (HKM).\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str, hkm_dimensionality: int):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.hkm_dimensionality = hkm_dimensionality\n",
    "\n",
    "        # 1. Load the pre-trained BERT model for Masked LM\n",
    "        self.bert_mlm = BertForMaskedLM.from_pretrained(model_name)\n",
    "        self.config = self.bert_mlm.config\n",
    "\n",
    "        # 2. Instantiate the Holographic Knowledge Manifold\n",
    "        self.hkm = HolographicKnowledgeManifold(d=hkm_dimensionality)\n",
    "\n",
    "        # 3. Create a projection head to map BERT's output to the HKM's dimension\n",
    "        bert_hidden_size = self.config.hidden_size # Should be 768 for bert-base\n",
    "        self.projection_head = nn.Linear(bert_hidden_size, hkm_dimensionality)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        \"\"\"\n",
    "        The forward pass for our custom model.\n",
    "        \"\"\"\n",
    "        # Pass inputs through the standard BERT model\n",
    "        # The BertForMaskedLM returns a dictionary-like object\n",
    "        outputs = self.bert_mlm(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            output_hidden_states=True # We need the hidden states for the HKM\n",
    "        )\n",
    "        \n",
    "        # The primary loss from the masked language modeling task\n",
    "        mlm_loss = outputs.loss\n",
    "\n",
    "        # Get the last hidden state from the BERT outputs\n",
    "        # Shape: (batch_size, sequence_length, hidden_size)\n",
    "        last_hidden_state = outputs.hidden_states[-1]\n",
    "\n",
    "        # For simplicity, we'll use the representation of the [CLS] token\n",
    "        # as the representation for the entire sequence.\n",
    "        # Shape: (batch_size, hidden_size)\n",
    "        cls_representation = last_hidden_state[:, 0, :]\n",
    "\n",
    "        # Project the CLS representation into the HKM's high-dimensional space\n",
    "        # Shape: (batch_size, hkm_dimensionality)\n",
    "        projected_vectors = self.projection_head(cls_representation)\n",
    "        \n",
    "        # During training, we would add these vectors to the HKM.\n",
    "        # For now, we just demonstrate the data flow.\n",
    "        # self.hkm.add_to_manifold(projected_vectors)\n",
    "        \n",
    "        # The model's output will be a dictionary containing the MLM loss\n",
    "        # and the projected vectors, which can be used for other tasks.\n",
    "        return {\n",
    "            \"loss\": mlm_loss,\n",
    "            \"projected_vectors\": projected_vectors\n",
    "        }\n",
    "\n",
    "print(\"Custom HGC-BERT model class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Architecture Verification\n",
    "\n",
    "To confirm that our architecture is correctly defined, we will now instantiate both the baseline model and our new custom model. We'll then print their structures to see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Baseline Model (BertForMaskedLM) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e0f504197084e8489c8c3d56c844264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66f720623bc84b239d19e5c0e3022dc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.56.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "\n",
      "Baseline model loaded successfully.\n",
      "Note: The full model is very large. We printed its config instead.\n",
      "\n",
      "--- HGC-Augmented Model (BertForMaskedLM_With_HGC) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM_With_HGC(\n",
      "  (bert_mlm): BertForMaskedLM(\n",
      "    (bert): BertModel(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0-11): 12 x BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSdpaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (cls): BertOnlyMLMHead(\n",
      "      (predictions): BertLMPredictionHead(\n",
      "        (transform): BertPredictionHeadTransform(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (transform_act_fn): GELUActivation()\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (hkm): HolographicKnowledgeManifold()\n",
      "  (projection_head): Linear(in_features=768, out_features=4096, bias=True)\n",
      ")\n",
      "\n",
      "HGC-augmented model loaded successfully.\n",
      "\n",
      "Verification of Projection Head Dimensions:\n",
      "  Input Features: 768\n",
      "  Output Features: 4096\n",
      "  Dimensions match correctly!\n"
     ]
    }
   ],
   "source": [
    "# -- 1. Instantiate the Baseline Model --\n",
    "print(\"--- Baseline Model (BertForMaskedLM) ---\")\n",
    "baseline_model = BertForMaskedLM.from_pretrained(MODEL_NAME)\n",
    "print(baseline_model.config)\n",
    "print(f\"\\nBaseline model loaded successfully.\")\n",
    "print(\"Note: The full model is very large. We printed its config instead.\")\n",
    "\n",
    "# -- 2. Instantiate the HGC-Augmented Model --\n",
    "print(\"\\n--- HGC-Augmented Model (BertForMaskedLM_With_HGC) ---\")\n",
    "hgc_model = BertForMaskedLM_With_HGC(\n",
    "    model_name=MODEL_NAME, \n",
    "    hkm_dimensionality=HKM_DIMENSIONALITY\n",
    ")\n",
    "print(hgc_model)\n",
    "print(f\"\\nHGC-augmented model loaded successfully.\")\n",
    "\n",
    "# Verify the dimensionality of the projection head\n",
    "proj_head = hgc_model.projection_head\n",
    "print(f\"\\nVerification of Projection Head Dimensions:\")\n",
    "print(f\"  Input Features: {proj_head.in_features}\")\n",
    "print(f\"  Output Features: {proj_head.out_features}\")\n",
    "assert proj_head.in_features == baseline_model.config.hidden_size\n",
    "assert proj_head.out_features == HKM_DIMENSIONALITY\n",
    "print(\"  Dimensions match correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusion\n",
    "\n",
    "We have successfully:\n",
    "1. Created a reusable, batch-friendly `HolographicKnowledgeManifold` module.\n",
    "2. Defined a custom `BertForMaskedLM_With_HGC` class that integrates the standard BERT model with our HKM via a projection head.\n",
    "3. Verified that both the baseline and custom models can be instantiated correctly from the `bert-base-uncased` checkpoint.\n",
    "\n",
    "This completes the architecture build step. We are now ready to move on to the next step: **Deploy (Training & Evaluation)**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
