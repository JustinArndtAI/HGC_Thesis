{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1, Step 1: Data Preparation\n",
    "\n",
    "This notebook handles the complete data preparation pipeline for Phase 1 of the HGC Thesis project. It performs the following steps:\n",
    "\n",
    "1.  **Task A Data (Broad Knowledge):** Fetches, cleans, and processes a corpus of articles on finance and economics from Wikipedia.\n",
    "2.  **Task B Data (Specialized Knowledge):** Loads, cleans, and processes a dataset of corporate earnings call transcripts.\n",
    "3.  **Structuring & Splitting:** Formats both datasets into a consistent structure, splits them into training, validation, and testing sets.\n",
    "4.  **Saving:** Saves the final, analysis-ready datasets to the `../data/` directory.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. Dependencies loaded and configuration set.\n"
     ]
    }
   ],
   "source": [
    "import wikipediaapi\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Configuration ---\n",
    "WIKI_LANG = 'en'\n",
    "DATA_DIR = '../data'\n",
    "TEST_SPLIT_SIZE = 0.15\n",
    "VALIDATION_SPLIT_SIZE = 0.15 \n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Ensure the data directory exists\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Setup complete. Dependencies loaded and configuration set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Task A: Fetch and Process Wikipedia Data (Finance & Economics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching content for 22 seed topics from Wikipedia...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Seed Topics: 100%|██████████████████████████████████████████████████████████| 22/22 [07:38<00:00, 20.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully fetched 1687 pages from Wikipedia.\n",
      "Performing initial cleaning...\n",
      "Wikipedia data cleaned. Final corpus size: 1672 documents.\n"
     ]
    }
   ],
   "source": [
    "wiki_wiki = wikipediaapi.Wikipedia(\n",
    "    language=WIKI_LANG,\n",
    "    user_agent=\"HGC_Thesis_Research/1.0 (justin.arndt@email.com)\", # Replace with your actual contact info\n",
    "    extract_format=wikipediaapi.ExtractFormat.WIKI\n",
    ")\n",
    "\n",
    "SEED_TOPICS = [\n",
    "    # Macroeconomics\n",
    "    'Macroeconomics', 'Fiscal policy', 'Monetary policy', 'Inflation', 'Gross domestic product', 'Unemployment',\n",
    "    # Microeconomics\n",
    "    'Microeconomics', 'Supply and demand', 'Market structure', 'Game theory', 'Opportunity cost',\n",
    "    # Financial Markets\n",
    "    'Stock market', 'Bond (finance)', 'Foreign exchange market', 'Derivative (finance)', 'Financial regulation',\n",
    "    # Corporate Finance\n",
    "    'Corporate finance', 'Capital budgeting', 'Valuation (finance)', 'Financial statement', 'Dividend policy',\n",
    "    'Mergers and acquisitions'\n",
    "]\n",
    "\n",
    "print(f\"Fetching content for {len(SEED_TOPICS)} seed topics from Wikipedia...\")\n",
    "\n",
    "all_pages = []\n",
    "processed_titles = set()\n",
    "\n",
    "# Check if Wikipedia data already exists to avoid re-downloading\n",
    "task_a_train_path = os.path.join(DATA_DIR, 'task_a_wikipedia_train.parquet')\n",
    "if os.path.exists(task_a_train_path):\n",
    "    print(\"Wikipedia data already processed. Skipping download.\")\n",
    "    wiki_df = pd.read_parquet(os.path.join(DATA_DIR, 'task_a_wikipedia_full_unsplit.parquet'))\n",
    "else:\n",
    "    for topic in tqdm(SEED_TOPICS, desc=\"Processing Seed Topics\"):\n",
    "        page = wiki_wiki.page(topic)\n",
    "        if page.exists() and topic not in processed_titles:\n",
    "            all_pages.append({'title': page.title, 'text': page.text, 'source': 'wikipedia'})\n",
    "            processed_titles.add(page.title)\n",
    "\n",
    "            # Get linked pages to broaden the corpus\n",
    "            for link in page.links:\n",
    "                if link not in processed_titles:\n",
    "                    linked_page = wiki_wiki.page(link)\n",
    "                    if linked_page.exists() and linked_page.text:\n",
    "                        all_pages.append({'title': linked_page.title, 'text': linked_page.text, 'source': 'wikipedia'})\n",
    "                        processed_titles.add(linked_page.title)\n",
    "    \n",
    "    wiki_df = pd.DataFrame(all_pages)\n",
    "    print(f\"Successfully fetched {len(wiki_df)} pages from Wikipedia.\")\n",
    "    print(\"Performing initial cleaning...\")\n",
    "\n",
    "    # --- Basic Cleaning ---\n",
    "    wiki_df['text_len'] = wiki_df['text'].str.len()\n",
    "    wiki_df = wiki_df[wiki_df['text_len'] > 500].copy()\n",
    "\n",
    "    def clean_wiki_text(text):\n",
    "        text = re.split(r'\\n==\\s?See also\\s?==', text, flags=re.IGNORECASE)[0]\n",
    "        text = re.split(r'\\n==\\s?References\\s?==', text, flags=re.IGNORECASE)[0]\n",
    "        text = re.split(r'\\n==\\s?External links\\s?==', text, flags=re.IGNORECASE)[0]\n",
    "        text = re.sub(r'\\n+', '\\n', text).strip()\n",
    "        return text\n",
    "\n",
    "    wiki_df['text'] = wiki_df['text'].apply(clean_wiki_text)\n",
    "    wiki_df = wiki_df[['text', 'source']].copy()\n",
    "    # Save the full unsplit data for caching\n",
    "    wiki_df.to_parquet(os.path.join(DATA_DIR, 'task_a_wikipedia_full_unsplit.parquet'), index=False)\n",
    "\n",
    "    print(f\"Wikipedia data cleaned. Final corpus size: {len(wiki_df)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Task B: Load and Process Earnings Call Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading earnings call transcript dataset from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c2a7b88243494fafd51c20d976b7c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09dd333c83ef4ae984246d8fdffd06cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "part-0.parquet:   0%|          | 0.00/1.82G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d8396f66b5d437b94aebf04533fb3d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/33362 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 33362 transcripts.\n",
      "Performing cleaning and structuring...\n",
      "Earnings call data cleaned. Final corpus size: 33248 documents.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading earnings call transcript dataset from Hugging Face...\")\n",
    "\n",
    "# FIX: Switched to a comprehensive and reliable dataset: 'kurry/sp500_earnings_transcripts'\n",
    "earnings_dataset = load_dataset(\"kurry/sp500_earnings_transcripts\", split='train')\n",
    "earnings_df = earnings_dataset.to_pandas()\n",
    "\n",
    "print(f\"Loaded {len(earnings_df)} transcripts.\")\n",
    "print(\"Performing cleaning and structuring...\")\n",
    "\n",
    "# The dataset has a 'content' column which we will rename to 'text'\n",
    "earnings_df.rename(columns={'content': 'text'}, inplace=True)\n",
    "earnings_df['source'] = 'earnings_call'\n",
    "\n",
    "# Clean up dataframe\n",
    "earnings_df = earnings_df[['text', 'source']].copy()\n",
    "earnings_df.dropna(inplace=True)\n",
    "earnings_df = earnings_df[earnings_df['text'].str.len() > 500] # Increased min length for quality\n",
    "\n",
    "print(f\"Earnings call data cleaned. Final corpus size: {len(earnings_df)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Split Datasets and Save to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_save(df, task_name):\n",
    "    print(f\"\\nSplitting and saving data for {task_name}...\")\n",
    "    \n",
    "    # First split: separate out the test set\n",
    "    train_val_df, test_df = train_test_split(\n",
    "        df, \n",
    "        test_size=TEST_SPLIT_SIZE, \n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    # Second split: separate the training and validation sets\n",
    "    # Adjusting the validation size relative to the remaining data\n",
    "    relative_val_size = VALIDATION_SPLIT_SIZE / (1 - TEST_SPLIT_SIZE)\n",
    "    train_df, val_df = train_test_split(\n",
    "        train_val_df, \n",
    "        test_size=relative_val_size, \n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    # Define file paths\n",
    "    train_path = os.path.join(DATA_DIR, f\"{task_name}_train.parquet\")\n",
    "    val_path = os.path.join(DATA_DIR, f\"{task_name}_val.parquet\")\n",
    "    test_path = os.path.join(DATA_DIR, f\"{task_name}_test.parquet\")\n",
    "    \n",
    "    # Save to Parquet format (more efficient than CSV)\n",
    "    train_df.to_parquet(train_path, index=False)\n",
    "    val_df.to_parquet(val_path, index=False)\n",
    "    test_df.to_parquet(test_path, index=False)\n",
    "    \n",
    "    print(f\"Data for {task_name} saved successfully:\")\n",
    "    print(f\"  Training set size:   {len(train_df)}\")\n",
    "    print(f\"  Validation set size: {len(val_df)}\")\n",
    "    print(f\"  Test set size:       {len(test_df)}\")\n",
    "\n",
    "# Process and save Task A data\n",
    "split_and_save(wiki_df, 'task_a_wikipedia')\n",
    "\n",
    "# Process and save Task B data\n",
    "split_and_save(earnings_df, 'task_b_earnings')\n",
    "\n",
    "print(\"\\n--- Data Preparation Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
