{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1, Step 1: Project Setup & Data Preparation\n",
    "\n",
    "**Objective:** To source, clean, and prepare two distinct, knowledge-intensive datasets for our continual learning experiment. \n",
    "\n",
    "- **Task A (Broad Knowledge):** Wikipedia articles on core finance and economics topics.\n",
    "- **Task B (Specialized Knowledge):** Corporate earnings call transcripts.\n",
    "\n",
    "This notebook will handle all preprocessing and save the final, analysis-ready datasets to the `../data/` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies\n",
    "\n",
    "First, we install and import all necessary libraries. We'll need `wikipedia` for Task A, `datasets` from Hugging Face for Task B, and standard data manipulation tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia-api\n",
      "  Downloading wikipedia_api-0.8.1.tar.gz (19 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: transformers in c:\\hgc_thesis\\venv\\lib\\site-packages (4.56.1)\n",
      "Requirement already satisfied: datasets in c:\\hgc_thesis\\venv\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: pandas in c:\\hgc_thesis\\venv\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: pyarrow in c:\\hgc_thesis\\venv\\lib\\site-packages (21.0.0)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.2-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: tqdm in c:\\hgc_thesis\\venv\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: requests in c:\\hgc_thesis\\venv\\lib\\site-packages (from wikipedia-api) (2.32.5)\n",
      "Requirement already satisfied: filelock in c:\\hgc_thesis\\venv\\lib\\site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\hgc_thesis\\venv\\lib\\site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\hgc_thesis\\venv\\lib\\site-packages (from transformers) (2.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\hgc_thesis\\venv\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\hgc_thesis\\venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\hgc_thesis\\venv\\lib\\site-packages (from transformers) (2025.9.1)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\hgc_thesis\\venv\\lib\\site-packages (from transformers) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\hgc_thesis\\venv\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\hgc_thesis\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\hgc_thesis\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\hgc_thesis\\venv\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in c:\\hgc_thesis\\venv\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\hgc_thesis\\venv\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\hgc_thesis\\venv\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\hgc_thesis\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\hgc_thesis\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\hgc_thesis\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.16.1-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: colorama in c:\\hgc_thesis\\venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\hgc_thesis\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\hgc_thesis\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\hgc_thesis\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\hgc_thesis\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\hgc_thesis\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\hgc_thesis\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\hgc_thesis\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\hgc_thesis\\venv\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: six>=1.5 in c:\\hgc_thesis\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\hgc_thesis\\venv\\lib\\site-packages (from requests->wikipedia-api) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\hgc_thesis\\venv\\lib\\site-packages (from requests->wikipedia-api) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\hgc_thesis\\venv\\lib\\site-packages (from requests->wikipedia-api) (2025.8.3)\n",
      "Downloading scikit_learn-1.7.2-cp311-cp311-win_amd64.whl (8.9 MB)\n",
      "   ---------------------------------------- 0.0/8.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/8.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/8.9 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.0/8.9 MB 2.8 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 2.6/8.9 MB 4.4 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 4.7/8.9 MB 6.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 6.0/8.9 MB 6.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.8/8.9 MB 6.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.1/8.9 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.9/8.9 MB 5.8 MB/s  0:00:01\n",
      "Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading scipy-1.16.1-cp311-cp311-win_amd64.whl (38.6 MB)\n",
      "   ---------------------------------------- 0.0/38.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.0/38.6 MB 12.7 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 3.4/38.6 MB 11.2 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 5.2/38.6 MB 10.0 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 7.1/38.6 MB 9.9 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 8.7/38.6 MB 9.6 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 10.7/38.6 MB 9.3 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 13.4/38.6 MB 9.5 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 16.0/38.6 MB 10.0 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 18.4/38.6 MB 10.1 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 20.7/38.6 MB 10.2 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 23.3/38.6 MB 10.5 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 26.2/38.6 MB 10.8 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 29.1/38.6 MB 11.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 32.2/38.6 MB 11.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 35.1/38.6 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.0/38.6 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.6/38.6 MB 11.6 MB/s  0:00:03\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Building wheels for collected packages: wikipedia-api\n",
      "  Building wheel for wikipedia-api (pyproject.toml): started\n",
      "  Building wheel for wikipedia-api (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for wikipedia-api: filename=wikipedia_api-0.8.1-py3-none-any.whl size=15542 sha256=ab0a2b3cc86a67c03aecd6225453a45a63e13d60c4044a30aab6b74672034501\n",
      "  Stored in directory: c:\\users\\data_\\appdata\\local\\pip\\cache\\wheels\\0b\\0f\\39\\e8214ec038ccd5aeb8c82b957289f2f3ab2251febeae5c2860\n",
      "Successfully built wikipedia-api\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, wikipedia-api, scikit-learn\n",
      "\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [scikit-learn]\n",
      "   ---------------------------------------- 5/5 [scikit-learn]\n",
      "\n",
      "Successfully installed joblib-1.5.2 scikit-learn-1.7.2 scipy-1.16.1 threadpoolctl-3.6.0 wikipedia-api-0.8.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install wikipedia-api transformers datasets pandas pyarrow scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project setup complete. Datasets will be saved to: C:\\HGC_Thesis\\data\n"
     ]
    }
   ],
   "source": [
    "import wikipediaapi\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_DIR = \"../data/\"\n",
    "WIKI_LANG = 'en'\n",
    "CHUNK_SIZE = 256  # Words per chunk\n",
    "CHUNK_OVERLAP = 50 # Words to overlap between chunks\n",
    "TEST_SIZE = 0.15\n",
    "VAL_SIZE = 0.15\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Ensure data directory exists\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "print(f\"Project setup complete. Datasets will be saved to: {os.path.abspath(DATA_DIR)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Task A: Wikipedia - Broad Financial & Economic Knowledge\n",
    "\n",
    "We will fetch content from a curated list of Wikipedia pages covering fundamental economic and financial concepts. This will form our initial, broad knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Wikipedia.__init__() missing 1 required positional argument: 'user_agent'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m wiki_wiki = \u001b[43mwikipediaapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mWikipedia\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mWIKI_LANG\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextract_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwikipediaapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mExtractFormat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mWIKI\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m SEED_TOPICS = [\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# Macroeconomics\u001b[39;00m\n\u001b[32m      8\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mMacroeconomics\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mFiscal policy\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mMonetary policy\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mInflation\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     24\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mMergers and acquisitions\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     25\u001b[39m ]\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFetching content for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(SEED_TOPICS)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seed topics from Wikipedia...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: Wikipedia.__init__() missing 1 required positional argument: 'user_agent'"
     ]
    }
   ],
   "source": [
    "wiki_wiki = wikipediaapi.Wikipedia(\n",
    "    language=WIKI_LANG,\n",
    "    extract_format=wikipediaapi.ExtractFormat.WIKI\n",
    ")\n",
    "\n",
    "SEED_TOPICS = [\n",
    "    # Macroeconomics\n",
    "    'Macroeconomics', 'Fiscal policy', 'Monetary policy', 'Inflation',\n",
    "    'Gross domestic product', 'Unemployment', 'Quantitative easing',\n",
    "    'Keynesian economics', 'Monetarism', 'Supply-side economics',\n",
    "\n",
    "    # Microeconomics\n",
    "    'Microeconomics', 'Supply and demand', 'Market structure',\n",
    "    'Perfect competition', 'Monopoly', 'Oligopoly', 'Game theory',\n",
    "    \n",
    "    # Financial Markets\n",
    "    'Financial market', 'Stock market', 'Bond market', 'Derivative (finance)',\n",
    "    'Efficient-market hypothesis', 'Capital asset pricing model',\n",
    "    'Behavioral economics', 'Foreign exchange market',\n",
    "\n",
    "    # Corporate Finance\n",
    "    'Corporate finance', 'Financial statement', 'Balance sheet', 'Income statement',\n",
    "    'Cash flow statement', 'Valuation (finance)', 'Discounted cash flow',\n",
    "    'Mergers and acquisitions'\n",
    "]\n",
    "\n",
    "print(f\"Fetching content for {len(SEED_TOPICS)} seed topics from Wikipedia...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_wiki_text(text):\n",
    "    \"\"\"Cleans Wikipedia text by removing headers, extra newlines, and references.\"\"\"\n",
    "    # Remove headers (e.g., == History ==)\n",
    "    text = re.sub(r'==.*?==+', '', text)\n",
    "    # Remove extra newlines\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    # Remove references that might be left over\n",
    "    text = re.sub(r'\\[\\d+\\]', '', text) \n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def chunk_text(text, chunk_size, overlap_size):\n",
    "    \"\"\"Splits text into overlapping chunks of a specified word count.\"\"\"\n",
    "    words = text.split()\n",
    "    if not words:\n",
    "        return []\n",
    "    \n",
    "    chunks = []\n",
    "    stride = chunk_size - overlap_size\n",
    "    for i in range(0, len(words), stride):\n",
    "        chunk = words[i:i + chunk_size]\n",
    "        if len(chunk) < chunk_size * 0.5 and len(chunks)>0: # Avoid very small trailing chunks\n",
    "            chunks[-1].extend(chunk)\n",
    "        else: \n",
    "             chunks.append(chunk)\n",
    "    \n",
    "    return [' '.join(chunk) for chunk in chunks]\n",
    "\n",
    "all_chunks = []\n",
    "for topic in tqdm(SEED_TOPICS, desc=\"Processing Wikipedia Articles\"):\n",
    "    page = wiki_wiki.page(topic)\n",
    "    if page.exists():\n",
    "        cleaned_text = clean_wiki_text(page.text)\n",
    "        chunks = chunk_text(cleaned_text, CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "        for chunk in chunks:\n",
    "            all_chunks.append({'text': chunk, 'source': 'wikipedia_finance'})\n",
    "\n",
    "task_a_df = pd.DataFrame(all_chunks)\n",
    "print(f\"Successfully created {len(task_a_df)} text chunks for Task A.\")\n",
    "task_a_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Task B: Earnings Call Transcripts - Specialized Financial Knowledge\n",
    "\n",
    "Next, we'll load a dataset of earnings call transcripts from the Hugging Face Hub. This data is highly specialized, full of jargon, and structurally different from Wikipedia, making it a perfect test for continual learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading Task B dataset from Hugging Face...\")\n",
    "# Using the 'presentation' part of earnings calls, which is dense with prepared statements.\n",
    "earnings_dataset = load_dataset(\"toughdata/quants\", split='train')\n",
    "print(\"Dataset loaded.\")\n",
    "\n",
    "task_b_chunks = []\n",
    "for item in tqdm(earnings_dataset, desc=\"Processing Earnings Calls\"):\n",
    "    # We focus on the prepared presentation section for dense knowledge\n",
    "    if item['section'] == 'presentation' and isinstance(item['segment'], str):\n",
    "        # The text is already quite clean, but we apply the same chunking for consistency\n",
    "        chunks = chunk_text(item['segment'], CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "        for chunk in chunks:\n",
    "            task_b_chunks.append({'text': chunk, 'source': 'earnings_calls'})\n",
    "\n",
    "task_b_df = pd.DataFrame(task_b_chunks)\n",
    "print(f\"Successfully created {len(task_b_df)} text chunks for Task B.\")\n",
    "task_b_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Splitting & Saving\n",
    "\n",
    "With both datasets processed and chunked, we'll now split them into training, validation, and testing sets. This ensures we can train our models, tune them on a validation set, and get a final, unbiased performance measure on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_save(df, task_name):\n",
    "    \"\"\"Splits a dataframe into train, validation, and test sets and saves them.\"\"\"\n",
    "    print(f\"\\nSplitting dataset for {task_name}...\")\n",
    "    \n",
    "    # First split: separate out the test set\n",
    "    train_val_df, test_df = train_test_split(df, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "    \n",
    "    # Second split: separate train and validation from the remaining data\n",
    "    # Adjusting the validation size relative to the remaining data\n",
    "    val_size_adjusted = VAL_SIZE / (1 - TEST_SIZE)\n",
    "    train_df, val_df = train_test_split(train_val_df, test_size=val_size_adjusted, random_state=RANDOM_STATE)\n",
    "    \n",
    "    print(f\"  Total examples: {len(df)}\")\n",
    "    print(f\"  Training set size: {len(train_df)}\")\n",
    "    print(f\"  Validation set size: {len(val_df)}\")\n",
    "    print(f\"  Test set size: {len(test_df)}\")\n",
    "    \n",
    "    # Save to parquet files for efficiency\n",
    "    train_df.to_parquet(os.path.join(DATA_DIR, f\"{task_name}_train.parquet\"))\n",
    "    val_df.to_parquet(os.path.join(DATA_DIR, f\"{task_name}_val.parquet\"))\n",
    "    test_df.to_parquet(os.path.join(DATA_DIR, f\"{task_name}_test.parquet\"))\n",
    "    \n",
    "    print(f\"  Successfully saved all sets for {task_name}.\")\n",
    "\n",
    "# Process Task A\n",
    "split_and_save(task_a_df, 'task_a')\n",
    "\n",
    "# Process Task B\n",
    "split_and_save(task_b_df, 'task_b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "We have successfully sourced, processed, and split our two datasets. The `../data/` directory now contains six Parquet files:\n",
    "\n",
    "- `task_a_train.parquet`, `task_a_val.parquet`, `task_a_test.parquet`\n",
    "- `task_b_train.parquet`, `task_b_val.parquet`, `task_b_test.parquet`\n",
    "\n",
    "This completes the data preparation step. We are now ready to move on to the next step: **building the HGC architecture and the baseline BERT model**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
