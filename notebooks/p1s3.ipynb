{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1, Step 3: Deploy (Training & Evaluation)\n",
    "\n",
    "**Objective:** To execute the full continual learning experiment, producing the core results that demonstrate the HGC architecture's resistance to catastrophic forgetting.\n",
    "\n",
    "This notebook will:\n",
    "1.  **Train Baseline & HGC Models on Task A:** Establish initial expertise.\n",
    "2.  **Update Models with Task B:** Use costly retraining for the baseline and near-zero-cost superposition for the HGC model.\n",
    "3.  **Evaluate All Models:** Test all model versions on both Task A and Task B test sets to quantify knowledge retention and acquisition.\n",
    "\n",
    "The final output will be a results table showing the perplexity scores, which will form the empirical basis for our paper and thesis chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "Load libraries, datasets, and define the training configuration. We'll set up everything needed for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    BertForMaskedLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "# Add project root to path to import our custom modules\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from hgc_core.hgc_module import HolographicKnowledgeManifold\n",
    "from notebooks.p1s2 import BertForMaskedLM_With_HGC # Assuming p1s2.ipynb contains the class\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "HKM_DIMENSIONALITY = 4096\n",
    "DATA_DIR = \"../data/\"\n",
    "RESULTS_DIR = \"../results/models/\"\n",
    "\n",
    "# GPU Performance Maximization\n",
    "BATCH_SIZE = 8  # Adjust based on your VRAM\n",
    "GRAD_ACCUM_STEPS = 4  # Effectively batch size of 32\n",
    "EPOCHS = 3\n",
    "\n",
    "# Ensure results directory exists\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load and Tokenize Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def load_and_tokenize(task_name):\n",
    "    datasets = {}\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        df = pd.read_parquet(os.path.join(DATA_DIR, f\"{task_name}_{split}.parquet\"))\n",
    "        # Convert pandas DataFrame to Hugging Face Dataset\n",
    "        hg_dataset = Dataset.from_pandas(df)\n",
    "        # Tokenize\n",
    "        tokenized_dataset = hg_dataset.map(\n",
    "            lambda examples: tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128),\n",
    "            batched=True, \n",
    "            remove_columns=['text', 'source']\n",
    "        )\n",
    "        datasets[split] = tokenized_dataset\n",
    "    return datasets\n",
    "\n",
    "print(\"Loading and tokenizing Task A...\")\n",
    "task_a_datasets = load_and_tokenize('task_a')\n",
    "print(\"Loading and tokenizing Task B...\")\n",
    "task_b_datasets = load_and_tokenize('task_b')\n",
    "\n",
    "# Data collator will dynamically handle masking for the MLM task\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "print(\"\\nData preparation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Part 1: Initial Training on Task A\n",
    "\n",
    "We train both the baseline and HGC models on the broad knowledge from Task A. For the HGC model, we use a custom Trainer to populate the HKM during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Training Arguments for all trainers\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(RESULTS_DIR, 'training_checkpoints'),\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    fp16=torch.cuda.is_available(), # Enable mixed-precision\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "# Custom Trainer for HGC model\n",
    "class HGCTrainer(Trainer):\n",
    "    def training_step(self, model, inputs):\n",
    "        # Standard forward pass and loss calculation\n",
    "        loss = super().training_step(model, inputs)\n",
    "        \n",
    "        # HGC-specific step: update the manifold\n",
    "        with torch.no_grad():\n",
    "            # We need to get the projected vectors from the model output\n",
    "            outputs = model(**inputs)\n",
    "            projected_vectors = outputs['projected_vectors'].detach()\n",
    "            model.hkm.add_to_manifold(projected_vectors)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "print(\"--- Training Baseline Model on Task A ---\")\n",
    "baseline_model_a = BertForMaskedLM.from_pretrained(MODEL_NAME)\n",
    "trainer_baseline = Trainer(\n",
    "    model=baseline_model_a,\n",
    "    args=training_args,\n",
    "    train_dataset=task_a_datasets['train'],\n",
    "    eval_dataset=task_a_datasets['val'],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "trainer_baseline.train()\n",
    "baseline_model_a.save_pretrained(os.path.join(RESULTS_DIR, 'baseline_task_a'))\n",
    "print(\"\\nBaseline model trained on Task A and saved.\")\n",
    "\n",
    "print(\"\\n--- Training HGC Model on Task A ---\")\n",
    "hgc_model_a = BertForMaskedLM_With_HGC(MODEL_NAME, HKM_DIMENSIONALITY)\n",
    "trainer_hgc = HGCTrainer(\n",
    "    model=hgc_model_a,\n",
    "    args=training_args,\n",
    "    train_dataset=task_a_datasets['train'],\n",
    "    eval_dataset=task_a_datasets['val'],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "trainer_hgc.train()\n",
    "torch.save(hgc_model_a.state_dict(), os.path.join(RESULTS_DIR, 'hgc_task_a.pt'))\n",
    "print(\"\\nHGC model trained on Task A and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Part 2: Knowledge Update with Task B\n",
    "\n",
    "Here we simulate the knowledge update. The baseline model is retrained, incurring significant cost. The HGC model is updated via fast, computationally cheap superposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Updating Baseline Model with Task B (Re-training) ---\")\n",
    "# The baseline trainer is already configured. We just call train again.\n",
    "trainer_baseline.train_dataset = task_b_datasets['train']\n",
    "trainer_baseline.eval_dataset = task_b_datasets['val']\n",
    "trainer_baseline.train()\n",
    "trainer_baseline.model.save_pretrained(os.path.join(RESULTS_DIR, 'baseline_task_ab'))\n",
    "print(\"\\nBaseline model updated with Task B and saved.\")\n",
    "\n",
    "print(\"\\n--- Updating HGC Model with Task B (Superposition) ---\")\n",
    "hgc_model_ab = BertForMaskedLM_With_HGC(MODEL_NAME, HKM_DIMENSIONALITY)\n",
    "hgc_model_ab.load_state_dict(torch.load(os.path.join(RESULTS_DIR, 'hgc_task_a.pt')))\n",
    "hgc_model_ab.eval() # Set model to evaluation mode\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "hgc_model_ab.to(device)\n",
    "\n",
    "update_loader = torch.utils.data.DataLoader(task_b_datasets['train'], batch_size=BATCH_SIZE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(update_loader, desc=\"HGC Superposition Update\"):\n",
    "        inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = hgc_model_ab(**inputs)\n",
    "        projected_vectors = outputs['projected_vectors'].detach()\n",
    "        hgc_model_ab.hkm.add_to_manifold(projected_vectors)\n",
    "\n",
    "torch.save(hgc_model_ab.state_dict(), os.path.join(RESULTS_DIR, 'hgc_task_ab.pt'))\n",
    "print(\"\\nHGC model updated with Task B and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Part 3: Final Evaluation & Results\n",
    "\n",
    "This is the final step. We evaluate all four model states (pre- and post-update) on both test sets. The results will definitively show the effect of catastrophic forgetting on the baseline and the knowledge retention of the HGC model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "model_paths = {\n",
    "    \"Baseline (Task A only)\": ('baseline_task_a', False),\n",
    "    \"HGC (Task A only)\": ('hgc_task_a.pt', True),\n",
    "    \"Baseline (Updated on B)\": ('baseline_task_ab', False),\n",
    "    \"HGC (Updated on B)\": ('hgc_task_ab.pt', True)\n",
    "}\n",
    "\n",
    "def evaluate_model(model, dataset):\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=TrainingArguments(output_dir=\"./eval_tmp\", per_device_eval_batch_size=BATCH_SIZE),\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "    eval_results = trainer.evaluate(eval_dataset=dataset)\n",
    "    return math.exp(eval_results['eval_loss'])\n",
    "\n",
    "for model_name, (path, is_hgc) in model_paths.items():\n",
    "    print(f\"\\n--- Evaluating: {model_name} ---\")\n",
    "    full_path = os.path.join(RESULTS_DIR, path)\n",
    "    \n",
    "    if is_hgc:\n",
    "        model = BertForMaskedLM_With_HGC(MODEL_NAME, HKM_DIMENSIONALITY)\n",
    "        model.load_state_dict(torch.load(full_path))\n",
    "    else:\n",
    "        model = BertForMaskedLM.from_pretrained(full_path)\n",
    "    \n",
    "    # Evaluate on Task A test set\n",
    "    ppl_a = evaluate_model(model, task_a_datasets['test'])\n",
    "    print(f\"  Perplexity on Task A Test Set: {ppl_a:.4f}\")\n",
    "    \n",
    "    # Evaluate on Task B test set\n",
    "    ppl_b = evaluate_model(model, task_b_datasets['test'])\n",
    "    print(f\"  Perplexity on Task B Test Set: {ppl_b:.4f}\")\n",
    "    \n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'Perplexity on Task A (Lower is Better)': ppl_a,\n",
    "        'Perplexity on Task B (Lower is Better)': ppl_b\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- FINAL RESULTS ---\")\n",
    "display(results_df)\n",
    "\n",
    "# Save results to a CSV for the paper/thesis\n",
    "results_df.to_csv(os.path.join(RESULTS_DIR, 'phase1_final_results.csv'), index=False)\n",
    "print(\"\\nResults saved to ../results/models/phase1_final_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
